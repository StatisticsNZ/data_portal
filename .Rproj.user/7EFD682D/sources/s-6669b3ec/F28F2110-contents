\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{xparse}

\definecolor{eclipseStrings}{RGB}{42,0.0,255}
\definecolor{eclipseKeywords}{RGB}{127,0,85}
\definecolor{jsonbackground}{RGB}{217, 237, 250}
\colorlet{numb}{magenta!60!black}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
language=R,
keywordstyle=\color{blue},
alsoletter={.},
otherkeywords={!=, ~, $, *, \&, \%/\%, \%*\%, \%\%, <-, <<-, /},
deletekeywords={c, data, get, aggregate, summary}
}





\NewDocumentCommand{\codeword}{v}{%
\texttt{\textcolor{blue}{#1}}%
}


\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    commentstyle=\color{eclipseStrings}, % style of comment
    stringstyle=\color{eclipseKeywords}, % style of strings
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    backgroundcolor=\color{jsonbackground}, %only if you like
    string=[s]{"}{"},
    comment=[l]{:\ "},
    morecomment=[l]{:"},
    literate=
        *{0}{{{\color{numb}0}}}{1}
         {1}{{{\color{numb}1}}}{1}
         {2}{{{\color{numb}2}}}{1}
         {3}{{{\color{numb}3}}}{1}
         {4}{{{\color{numb}4}}}{1}
         {5}{{{\color{numb}5}}}{1}
         {6}{{{\color{numb}6}}}{1}
         {7}{{{\color{numb}7}}}{1}
         {8}{{{\color{numb}8}}}{1}
         {9}{{{\color{numb}9}}}{1}
}

\include{defs}

\newcommand\statssection[1]{%
  \section{{#1}}
  \raisebox{1.5em}[0pt][0pt]{\textcolor{statsorange}{\rule{0.2\textwidth}{6pt}}}
}


%%%%%%%%%%%%%%%
% Title Page
\title{Generic data portals}
\author{Documentation and process outline}
\date{\today}
%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\tableofcontents
\clearpage
\setcounter{page}{1}

\vspace{0.4cm}

\statssection{Introduction}

The publication of a large number of unique data sets in a user friendly visualisation can present a challenging problem.  This is often solved through the production of a specialised \textit{dashboard} for the particular data set and audience.  However, these solutions can become complicated, expensive to maintain and can be difficult to recycle for new data.  This is often because dashboards are built to be highly specific and over-engineered for the given data set, where a desire to show a large number of unique visualisations has limited the the ability to generalise the software for other uses.
\vspace{0.5cm}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/data_portal.png}
 	\caption{The Stats NZ COVID-19 Data Portal which runs using the software presented in this document.}\label{fig:covid_19_portal}
\end{figure}

The advantage of dashboards over a traditional publishing process is that they enable the user to generate insights based on their needs.  Data sets may come with many regional, industry and social (to name a few) dimensions that are difficult to properly represent through a choice of only a handful of graphs.  There is a need for a solution which enables rapid publication of large numbers of different data sets in a semi-automated but flexible way.

We present here a simple solution for these problems.  This data portal software is a framework for building a range of similar data dissemination tools, with a robust data management process combined with sufficient flexibility to work with a range of data sources and visualisation requirements.

This software is the basis of the COVID-19 Data Portal, given in Figure \ref{fig:covid_19_portal}, which was published on the experimental Stats NZ website in April 2020.  The structure of the code has enabled this portal to grow to holding over 300 different time series from a large range of different input formats.  The code was further generalised into the current state in May 2020 to allow for quick deployment of similar portals.  In Figure \ref{fig:env_data_portal} we show the same software used to display environmental data directly from a web based data service.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/data_portal_alt.png}
 	\caption{The data portal software used to create a visualisation of environmental data from a web based data service.}\label{fig:env_data_portal}
\end{figure}

This documentation describes the data management principles and software.  The intended use of this documentation, and the associated code, is for the reuse of this solution for rapid production of data visualisations.

\clearpage
\statssection{The data management process}\label{sec:data}
\begin{wrapfigure}{r}{0.5\textwidth}
\centering
\includegraphics[width=0.5\textwidth]{figures/data_process.pdf}
 	\caption{The data ingestion process.  Load from internal datebase not yet implemented (trivial to set up).}\label{fig:data_process}
\end{wrapfigure}
For this application to be able to scale quickly and be reused for a range of needs there must be a robust and flexible data management process.  In Figure \ref{fig:data_process} we show the process map for the various types of data sources and how these flow through the system.

In Figure \ref{fig:data_process} we show four possible sources of data at the top.  These represent the first steady-state.  The original raw data is read in using either one of the predefined functions, or by writting a custom function if a new source is required (such as a new web service).  For a file this is as simple as pointing to the correct directory, so there is little abstraction required -- but web services or database calls may require specific functions.  This raw data is then passed into the transformation layer.  This layer may contain many custom \textit{load functions} designed to transform raw data into a consistent format.  The output of these load functions must be a type defined in \codeword{R/data_types.R}.  If a suitable structure is not available then one can be defined in that file.

Internal data is then passed into the \textit{data store}.  This is simple list containing all the data sets saved into a file on the harddrive.  This file is then deployed with the application.

Finally all data is accessible through the client-service interface.  From the perspective of the client this layer is called in the same way with the same behaviour regardless of the data source, returning one of the expected data types.  This data is then passed through the visualisation layer, where custom plot functions may be defined for alternative visualisations of data.

See Section \ref{sec:custom_functions} for details on how to build functions for each of the layers in this process.

\newpage
\clearpage
\statssection{Code documentation}

This section outlines how to work with data portal software including how to define new visualisations and create custom functions for the data management process.  The generic data portal software can be downloaded from:\\

\noindent
\url{https://github.com/StatisticsNZ/data_portal}

\subsection{Examples}

To get started with the generic data portal code it is good to try the examples.  To run the most basic example:
\begin{enumerate}
\item Copy the file \codeword{config/example/example_config.yaml} into a new file called \codeword{config/config.yaml} (this file is not to be kept under version control as it's specific to each portal).
\item Run the script \codeword{scripts/run_load_process.R} to load the data into an RDS file.
\item Run the command \codeword{shiny::runApp(".")} to start the application.
\end{enumerate}

This will use the indicator and data definitions defined in \codeword{config/example_indicators.json} and \codeword{config/example_data_definitions.json} to create a visualisation from the data in the \codeword{example_data/} folder.  These can be changed in the \codeword{config/config.yaml} file, see Section \ref{sec:config_file} for details.  To run a more sophisticated example that uses data purely from a web based service run the application with the following steps.
\begin{enumerate}
\item Copy the file \codeword{config/api_example/api_example_config.yaml} into \codeword{config/config.yaml}
\item Run the command \codeword{shiny::runApp(".")} to start the application.
\end{enumerate}
Note that in this case we did not need to load to an RDS file as all data comes from a web service.

\newpage
\clearpage
\subsection{The configuration file}\label{sec:config_file}

The configuration file is a YAML file of key-value pairs that is loaded in at the start of the application.  This file controls the look and feel of the application and defines the file paths to other configurable parts of the application, along with any other adjustable parameters.  The file must have the path \codeword{config/config.yaml}.  This avoids the need for any definitions in the R code itself, making it easier to maintain and configure.  An example of the configuration file contents in given in Figure \ref{fig:config_file}.

The configuration parameters and meaning are:
\begin{itemize}
\item \codeword{title} - The name to appear in the about and download modals.
\item \codeword{production} - for data coming from files on the file system read from the files at Shiny application start up (if \codeword{false}) otherwise read from the RDS file if (\codeword{true}) (requires running the load data script first to create the RDS file).
\item \codeword{data_directory} - If reading data from the file system what directory are the files stored in (\textit{optional}).
\item \codeword{indicator_definitions} - file path to the indicator definitions JSON file (see Section \ref{sec:indicator_definition}.
\item \codeword{data_definitions} - file path to the data definitions JSON file (see Section \ref{sec:data_definition} (\textit{optional}).
\item \codeword{default_parameters} - a list of default parameters to use if these are not found when looking in the indicator or data definition files (\textit{optional}).
\item \codeword{primary_color} - the brand color for the application.
\item \codeword{data_store_filename} - the RDS file for storing data from the file system (used for both reading and writing) (\textit{optional}).
\item \codeword{about_modal_html} - HTML file for the content to appear in the about modal.
\item \codeword{download_modal_html} - HTML file for the content to appear in the download modal.
\item \codeword{tag_manager_html} - HTML file containing the Google Analytics tag manager code (\textit{optional}).
\end{itemize}


\begin{figure}
\footnotesize
\begin{lstlisting}[language=json,firstnumber=1]
title: "COVID-19"
production: true
data_directory:  "~/COVID-19 data_Secure/COVID-19_dashboard/"
indicator_definitions: "config/covid_19/covid_19_indicators.json"
data_definitions: "config/covid_19/covid_19_data_definitions.json"
default_parameters:
  data_type: "TimeSeries"
  plot_function: "get_time_series_plot"
  data_service: "load_from_store"
primary_color: "#EC6607"
data_store_filename: "data_store.RDS"
about_modal_html: "www/about_covid_19.html"
download_modal_html: "www/download_modal_covid_19.html"
tag_manager_html: "www/tag_manager.html"
\end{lstlisting}
\caption{Example configuration file.  This file controls the overall appearance of the application and defines the location of various other files which are specific to the particular product.}\label{fig:config_file}
\end{figure}

\subsection{Defining indicators}\label{sec:indicator_definition}

Indicators are defined in a JSON format configuration file.  For each indicator a block of JSON defines all properties such as the title, labels, the source of the data and any additional parameters required to request that data.

All visualisations (each chart of graph) in the application must have a unique key, which is a concatenation of
\begin{itemize}
\item The \codeword{class}
\item The \codeword{type}
\item The \codeword{indicator_name}
\item The \codeword{name} parameter of the group
\end{itemize}
These parameters are defined in the corresponding JSON, with an example given in Figure \ref{fig:indicator_definition}.  These four parameters will uniquely define the tab selected, the three choices of the drop down selectors at the top of the page and as a result a data source and a visualisation.

The possible parameters for an indicator definition are:
\begin{itemize}
\item \codeword{class} -- The tab the indicator will appear on.
\item \codeword{type} -- The first drop down selector.
\item \codeword{indicator_name} -- The second drop down selector.
\item \codeword{title} -- the title to appear on the graph.
\item \codeword{source} -- the name of the data source.
\item \codeword{source_url} -- the URL for the data source.
\item \codeword{plot_function} -- the name of the plot function.
\item \codeword{international} -- \codeword{true} or \codeword{false} will determine grouping in indicator list.
\item \codeword{download} --  \codeword{true} to include this indicator in the download CSV file.
\item \codeword{data_service} -- the data service to use to fetch the data.
\item \codeword{include_date_slider} -- \codeword{true} to include a data slider.
\item \codeword{default_lower_range} -- the lower date range in the format ``YYYY-MM-DD''.
\item \codeword{caveats} -- HTML or text for contents of caveat box (text block below graph with orange border).
\item \codeword{description} --  HTML or text for contents of description box (text block below graph).
\item \codeword{groups} -- an array of JSON blocks with the following parameters:
\begin{itemize}
\item \codeword{name} -- the option in the third drop down box.
\item \codeword{title} -- the title (if different to above).
\item \codeword{units} -- the units to appear on the y-axis label.
\item \codeword{data_service} -- the data service (if different to above).
\item \codeword{caveats} -- caveats text or HTML (if different to above).
\item \codeword{description} -- description text or HTML (if different to above).
\end{itemize}
\end{itemize}

Parameters defined in the indicator definition are generally returned using the function \codeword{get_indicator_parameter}.  This function will first check the \codeword{group} block of parameters for the required parameter, and if it is not found, will use the value at the indicator level.  This allows for specific parameters to be applied at the group level when necessary.

\begin{figure}[h!]
\footnotesize
\begin{lstlisting}[language=json,firstnumber=1]
  {
    "class": "Economic",
    "type": "Transport",
    "indicator_name": "Flight departures by main airports",
    "source": "Flightradar24",
    "plot_function": "get_time_series_plot",
    "international": false,
    "source_url": "https://www.flightradar24.com/data/statistics",
    "download": false,
    "groups": [
      {
        "name": "Auckland Airport",
        "title": "Daily departures - Auckland Airport",
        "units": "Number"
      },
      {
        "name": "Wellington Airport",
        "title": "Daily departures - Wellington Airport",
        "units": "Number"
      }
    ]
  }
\end{lstlisting}
\caption{Example indicator definition.  This block of JSON defines everything about the indicator including how it will look in the application and where the data will come from.}\label{fig:indicator_definition}
\end{figure}


\subsection{Reading data from the file system}\label{sec:data_definition}

Data may be read from the file system prior to deployment of the application and stored in a temporary data store.  This data store is a single file with all data stored in a consistent format (one of the data models defined in \codeword{R/data_types.R}).  This is preferred over deploying the application with many different files as the initial load can be extended significantly due to the time to read these in.  To load files into the data store before deployment the data sets must be defined in a \codeword{data_definitions.json} file.  In Figure \ref{fig:data_definition} we show the example corresponding to the indicator defined in Figure \ref{fig:indicator_definition}.  Like the indicators, each data set can be read into a unique key-value pair in the data store, defined uniquely by the \codeword{class}, \codeword{type}, \codeword{indicator_name} and \codeword{group_names}.  Note that in this example the same file will be read into two different key-value pairs, corresponding to the two unique visualisations defined in Figure \ref{fig:indicator_definition}.

It is also possible to add data into the same key-value pair in the data store from multiple files.  In the \codeword{data_types.R} file there are addition operators defined for the R6 classes holding the data, and these will be employed if there is another definition in the \codeword{data_definitions.json} file which points more data at an existing key-value pair.  However, this will obviously fail if the \codeword{data_type} parameters are different.

The possible parameters in the \codeword{data_definitions.json} file varies depending on the load function used.  For new load functions we are trying to put all load function specific parameters into a separate group called \codeword{load_parameters} to make it clear these are specific to that function, although this is not a global rule yet.  Some common parameters are:
\begin{itemize}
\item \codeword{class} -- The tab the indicator will appear on.
\item \codeword{type} -- The first drop down selector.
\item \codeword{indicator_name} -- The second drop down selector.
\item \codeword{load_function} -- the name of the load function to use to read the file.
\item \codeword{filename} -- the file name.
\item \codeword{data_type} -- the data type, such as ``TimeSeries''.
\end{itemize}
Some common parameters which are used in the \codeword{read_from_excel} load function and appear in most of our data definitions:
\begin{itemize}
\item \codeword{sheet_number} -- the sheet in the Excel file to read from.
\item \codeword{parameter_col} -- the column which contains the independent variable, such as the date for a time series or category for a bar chart.
\item \codeword{value_cols} -- the columns which hold the data values.
\item \codeword{value_names} -- the names for each set of values - will appear in the legend of the graph and of particular importance when there will be multiple lines on the same graph.
\item \codeword{group_names} -- the string which corresponds to the name of the group block in the indicator definition.  This will generally match the \codeword{value_names} or be the same for all values if you want to assign multiple lines to one graph.
\item \codeword{drop_na} -- if \codeword{true} will drop rows of data that have \codeword{NA} values.
\item \codeword{skip} -- lines to skip at the start of the sheet.
\item \codeword{input_units} -- array (if different for each column) or single value for the scaling of values.  For example if set to 1000 all values will be multiplied by 1000 on when loaded.
\item \codeword{parameter_transform} -- an inline R function defining how to convert the parameter into the desired for, for dates this is often something like ``function (x) ymd(x)''.
\end{itemize}

\begin{figure}
\footnotesize
\begin{lstlisting}[language=json,firstnumber=1]
      {
        "class": "Economic",
        "indicator_name": "Flight departures by main airports",
        "type": "Transport",
        "parameter_col": 1,
        "parameter_transform": "function(x) ymd(x)",
        "sheet_number": 1,
        "value_col": [2, 3, 4],
        "value_names": ["Auckland Airport", "Wellington Airport"],
        "group_names": ["Auckland Airport", "Wellington Airport"]
        "filename": "Daily flight departures.xlsx",
        "load_function": "read_from_excel",
        "data_type": "TimeSeries"
      }
\end{lstlisting}
\caption{Data definition example.  This block of JSON tells the application how to load and transform data from a file into the RDS store.}\label{fig:data_definition}
\end{figure}


\subsection{Custom functions}\label{sec:custom_functions}

This code is designed to provide a robust structure for data management while allowing maximum flexibility in both the input data and the visualisations.  In Figure \ref{fig:data_process} we show the customisable layers in the process flow.  Each of these layers can be modified using custom functions, such as custom load functions in the transform layer, plot functions in the visualisation layer, or even entirely new data sources in the very first layer.  It is also possible to construct new data models, if the TimeSeries or BarChart classes do suit your needs (for example geographical data may need a new data model to be created).  In this section we describe the customisable parts of the code.

\subsubsection{Load functions}

Load functions are used to read data from a range disparate file formats.  If particular data set is going to be delivered regularly in a consistent format it is preferable to simply write a custom load function, rather than manually changing the file or requesting that your data supplier does it for you (which can create burden on them, ultimately slowing down supply and potentially limiting the likelihood of being provided more comprehensive data in future).  So we have the ability to write short functions for handling the different formats we can encounter.

The load functions are currently defined in \codeword{R/load_functions.R}.  When a new load function is defined it must be added to the list at the end of this file so it can be referenced by a key, as defined in the configuration file. Each load function must return a named list of data objects (of a type defined in \codeword{R/data_types.R} -- see section \ref{sec:data_models}), with names equal to the name of the group defined in the indicator definition that it corresponds to.

The internal details of a load function are entirely up to the developer, as long as it takes in a configuration object (from the \codeword{data_definition.json} file or equivalent) and returns a named list of data objects.  However, it is recommended that if a load function can be generalised, and code reused for other indicators with a simple change in input parameters, then this should be done to avoid create a large amount of additional code to maintain.

\subsubsection{Data models}\label{sec:data_models}

Currently we have two data models, a TimeSeries and a BarChart, they are defined in R6 classes in \codeword{R/data_types.R}.  These classes give additional structure to our data, with every indicator having to conform to one of these types.  The classes also have methods for extracting the data to go in the download CSV.

New data models can be created in this file.  The only considerations are that they need to have a method \codeword{get_csv_content} which returns the data in a long table format with a column for sub-series name.  The way data models with with the download function needs to be extended to enable more generic types, as the current system is somewhat rigid.

\subsubsection{Data services}

Data services define the first line in our process map in Figure \ref{fig:data_process}.  We currently have implemented \codeword{load_from_store} which reads from the saved RDS file deployed with the application, \codeword{load_from_web_service}, an example of calling a basic web service and \codeword{load_from_environmental_data} which demonstrates how to call data from a well structured API, along with some manipulation to the received data.

Web based data service functions may also make use of load functions, which can be defined in the usual way.

To define the data service change the \codeword{data_service} parameter in the indicator definition to a function defined in \codeword{R/data_service_functions.R}.  In these functions we define how to call a specific web service and interpret the data, returning a data object that conforms to one of our data models.

\subsection{Custom visualisations}

It is possible to display any type of visualisation in this application by defining custom plot functions.  The plot functions are defined in \codeword{R/plot_functions.R}.  All that is required is a function which takes in one of the accepted data models (a new one can be created if required) and returns a highcharter object.
%
%\section{The production process}
%
%The current COVID-19 dashboard may be updated multiple times per day depending on the supply of data.  Generally, one update is run in the mid-afternoon once a number of daily indicators have been delivered.
%
%\subsection{The process for updating data}
%\begin{enumerate}
%\item Data is collected from a range of sources, which includes emails, automated loading into the file system or collection from external websites.
%\item All data is put into a location on the network file system.
%\item The script \codeword{scripts/run_load_process.R} is run to read all relevant files and store them in the \codeword{data_store.RDS} file ready for deployment.
%\item The application is run locally in production mode for a ``smoke test'' and check that data has updated, including a check of the download functionality.
%\item The application is deployed to the production environment at:\\
%\url{https://statisticsnz.shinyapps.io/covid\_19\_dashboard/}
%\end{enumerate}
%
%\subsection{The process for adding new indicators or code changes}
%\begin{enumerate}
%\item The relevant code changes are made and tested locally.
%\item Code changes are committed to our version control system and pushed to GitLab.
%\item The application is deployed to the staging environment at:\\
%\url{https://statisticsnz.shinyapps.io/covid\_19\_dashboard\_staging/}
%\item The application is inspected thoroughly by two people looking at both the new indicators and a range of existing indicators.
%\item Any fixes are pushed to the version control system.
%\item The application is deployed to the production environment at:\\
%\url{https://statisticsnz.shinyapps.io/covid\_19\_dashboard/}
%\end{enumerate}
%




\clearpage% Clear first
\thispagestyle{empty}



\end{document}          
